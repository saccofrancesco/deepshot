{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "In this section, we will set up the necessary environment for training optimizing a machine learning model using a XGBoost Classifier classifier and **Bayesian Search**. We will:\n",
    "\n",
    "1. **Import Essential Libraries** – Load key Python libraries for data handling, model training, evaluation, and hyperparameter tuning.\n",
    "2. **Load and Prepare the Dataset** – Read the dataset from a CSV file, remove unnecessary columns, and split the data into features (`X`) and target labels (`y`).\n",
    "3. **Perform Data Splitting** – Divide the dataset into training and testing sets to ensure the model generalizes well to unseen data.\n",
    "4. **Define the Hyperparameter Search Space** – Specify a range of values for key hyperparameters of the Random Forest model to optimize performance.\n",
    "5. **Optimize Model with Bayesian Search** – Utilize Bayesian optimization via `BayesSearchCV` to efficiently search for the best hyperparameters.\n",
    "6. **Evaluate the Model** – Assess the model's performance using accuracy and a classification report.\n",
    "\n",
    "The entire process will be logged with **Rich Console** to enhance readability and provide real-time updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading Data...\")\n",
    "df: pd.DataFrame = pd.read_csv(\"../data/csv/dataset.csv\")\n",
    "print(\"Data Loaded Successfully!\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only rows from 2023-10-24 and later\n",
    "df: pd.DataFrame = df[df[\"date\"] >= \"2021-10-19\"].reset_index(drop=True)\n",
    "\n",
    "# Drop non-training columns\n",
    "df: pd.DataFrame = df.drop([\"date\", \"home_team\", \"away_team\"], axis=1)\n",
    "X: pd.DataFrame = df.drop(\"winning_team\", axis=1)\n",
    "y: pd.DataFrame = df[\"winning_team\"]\n",
    "\n",
    "# Applying SMOTE for oversapling the minority feature (winning_team = 1)\n",
    "X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X, y)\n",
    "\n",
    "# Split dataset\n",
    "print(\"Splitting the dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial) -> float:\n",
    "    params: dict[str, int | float | str] = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 20),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    model: XGBClassifier = XGBClassifier(**params)\n",
    "\n",
    "    score: np.ndarray = cross_val_score(\n",
    "        model, X_train, y_train, scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    "    )\n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "# Create and run study\n",
    "print(\"Starting Optuna hyperparameter tuning...\")\n",
    "study: optuna.Study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=500, n_jobs=-1)\n",
    "\n",
    "# Print best trial\n",
    "print(\"\\nBest Trial:\")\n",
    "print(f\"  Value: {study.best_trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Train final model\n",
    "best_params: dict[str, int | float | str] = study.best_trial.params\n",
    "best_params.update(\n",
    "    {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "final_model: XGBClassifier = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred: np.ndarray = final_model.predict(X_test)\n",
    "accuracy: float = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nFinal Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualization 1: Optimization History\n",
    "# Plot optimization history\n",
    "plt.figure(figsize=(8, 6))\n",
    "opt_history: list[float] = [trial.value for trial in study.trials]\n",
    "plt.plot(opt_history)\n",
    "plt.xlabel(\"Trial\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.title(\"Optuna Optimization History\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Hyperparameter Importance\n",
    "# Plot hyperparameter importance\n",
    "param_importance: dict[str, float] = study.best_trial.params\n",
    "params, importances = zip(*param_importance.items())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(params, importances)\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Hyperparameters\", fontsize=14)\n",
    "plt.title(\"Hyperparameter Importance\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
