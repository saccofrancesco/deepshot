{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with Optuna Hyperparameter Tuning\n",
    "\n",
    "#### Overview\n",
    "This notebook implements a machine learning pipeline to predict NBA game outcomes using XGBoost with automated hyperparameter optimization via Optuna.\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "##### 1. Data Preparation\n",
    "- Load NBA game dataset from CSV\n",
    "- Filter games from October 19, 2021 onwards\n",
    "- Remove non-predictive features (date, team names)\n",
    "- Separate features (X) and target variable (winning_team)\n",
    "\n",
    "##### 2. Class Imbalance Handling\n",
    "- Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset\n",
    "- Ensures the model doesn't bias toward the majority class\n",
    "\n",
    "##### 3. Hyperparameter Optimization\n",
    "- Use Optuna framework for Bayesian optimization\n",
    "- Search space includes:\n",
    "  - Tree structure: `n_estimators`, `max_depth`, `min_child_weight`\n",
    "  - Learning dynamics: `learning_rate`, `subsample`, `colsample_bytree`\n",
    "  - Regularization: `gamma`, `reg_alpha`, `reg_lambda`\n",
    "- 500 trials with 5-fold cross-validation\n",
    "- Maximize accuracy metric\n",
    "\n",
    "##### 4. Model Training & Evaluation\n",
    "- Train final XGBoost classifier with optimal parameters\n",
    "- Evaluate on held-out test set (20% split)\n",
    "- Visualize optimization history and hyperparameter importance\n",
    "\n",
    "#### Expected Outputs\n",
    "1. Best hyperparameters and cross-validation score\n",
    "2. Final model test accuracy\n",
    "3. Optimization history plot\n",
    "4. Hyperparameter importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "# Suppress Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading Data...\")\n",
    "df: pd.DataFrame = pd.read_csv(\"../data/csv/dataset.csv\")\n",
    "print(\"Data Loaded Successfully!\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only rows from 2021-10-19 and later\n",
    "df: pd.DataFrame = df[df[\"date\"] >= \"2021-10-19\"].reset_index(drop=True)\n",
    "\n",
    "# Drop non-training columns\n",
    "df: pd.DataFrame = df.drop([\"date\", \"home_team\", \"away_team\"], axis=1)\n",
    "X: pd.DataFrame = df.drop(\"winning_team\", axis=1)\n",
    "y: pd.Series = df[\"winning_team\"]\n",
    "\n",
    "# Split dataset BEFORE applying SMOTE to prevent data leakage\n",
    "print(\"Splitting the dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Applying SMOTE for oversampling the minority class (winning_team = 1)\n",
    "# Apply only to training data to prevent data leakage\n",
    "print(\"Applying SMOTE to balance training data...\")\n",
    "smote: SMOTE = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Resampled training set: {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective function for optimizing an XGBoost classifier.\n",
    "\n",
    "    This function defines the hyperparameter search space for an\n",
    "    `XGBClassifier`, trains the model using cross-validation, and returns\n",
    "    the mean accuracy score to be maximized by Optuna.\n",
    "\n",
    "    Hyperparameters are sampled dynamically by the provided Optuna trial.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.trial.Trial\n",
    "        Optuna trial object used to suggest hyperparameter values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean cross-validated accuracy score across all folds.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Five-fold cross-validation is used to evaluate each trial.\n",
    "    - Accuracy is used as the optimization metric.\n",
    "    - Training data (`X_train_resampled`, `y_train_resampled`) is expected\n",
    "      to be available in the enclosing scope.\n",
    "    - The model objective is binary classification using logistic loss.\n",
    "    - Parallel execution is enabled via `n_jobs=-1`.\n",
    "    \"\"\"\n",
    "    params: dict[str, int | float | str] = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 20),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    model: XGBClassifier = XGBClassifier(**params)\n",
    "\n",
    "    score: np.ndarray = cross_val_score(\n",
    "        model, X_train_resampled, y_train_resampled, scoring=\"accuracy\", cv=5, n_jobs=-1\n",
    "    )\n",
    "    return float(np.mean(score))\n",
    "\n",
    "\n",
    "# Create and run study with pruning\n",
    "print(\"\\nStarting Optuna hyperparameter tuning...\")\n",
    "study: optuna.Study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    ")\n",
    "study.optimize(objective, n_trials=500, n_jobs=-1, show_progress_bar=True)\n",
    "\n",
    "# Print best trial\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST TRIAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Cross-Validation Accuracy: {study.best_trial.value:.4f}\")\n",
    "print(\"\\nOptimal Hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\nTraining final model with optimal hyperparameters...\")\n",
    "best_params: dict[str, int | float | str] = study.best_trial.params.copy()\n",
    "best_params.update(\n",
    "    {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "final_model: XGBClassifier = XGBClassifier(**best_params)\n",
    "final_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred: np.ndarray = final_model.predict(X_test)\n",
    "y_pred_proba: np.ndarray = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print comprehensive metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Away Win\", \"Home Win\"]))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Visualization 1: Optimization History\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "opt_history: list[float] = [trial.value for trial in study.trials]\n",
    "plt.plot(opt_history, linewidth=2, alpha=0.7)\n",
    "plt.axhline(\n",
    "    y=study.best_trial.value,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Best: {study.best_trial.value:.4f}\",\n",
    ")\n",
    "plt.xlabel(\"Trial\", fontsize=14)\n",
    "plt.ylabel(\"Cross-Validation Accuracy\", fontsize=14)\n",
    "plt.title(\"Optuna Optimization History\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/optimization_history.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Hyperparameter Importance (FIXED)\n",
    "param_importance: dict[str, float] = get_param_importances(study)\n",
    "params: list[str] = list(param_importance.keys())\n",
    "importances: list[float] = list(param_importance.values())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(params, importances, color=\"steelblue\", alpha=0.8)\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Hyperparameters\", fontsize=14)\n",
    "plt.title(\"Hyperparameter Importance\", fontsize=16, fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/hyperparameter_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 3: Confusion Matrix\n",
    "cm: np.ndarray = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Away Win\", \"Home Win\"],\n",
    "    yticklabels=[\"Away Win\", \"Home Win\"],\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.title(\"Confusion Matrix\", fontsize=16, fontweight=\"bold\")\n",
    "plt.ylabel(\"True Label\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 4: ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc: float = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random Classifier\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/roc_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization 5: Feature Importance\n",
    "feature_importance: np.ndarray = final_model.feature_importances_\n",
    "feature_names: pd.Index = X_train.columns\n",
    "\n",
    "# Get top 20 features\n",
    "indices: np.ndarray = np.argsort(feature_importance)[::-1][:20]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(\n",
    "    range(len(indices)),\n",
    "    feature_importance[indices],\n",
    "    color=\"teal\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Feature Importance\", fontsize=14)\n",
    "plt.ylabel(\"Features\", fontsize=14)\n",
    "plt.title(\"Top 20 Most Important Features\", fontsize=16, fontweight=\"bold\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"models/feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll visualizations saved to models/ directory\")\n",
    "print(\"\\nâœ“ Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
